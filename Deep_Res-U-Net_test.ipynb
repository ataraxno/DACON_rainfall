{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.003\n",
    "DECAY_STEPS = 1000\n",
    "DECAY_RATE = 2\n",
    "EPSILON = 1e-06\n",
    "BEST_PATH = './models/DeepResUNet.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = glob.glob('./data/train/*.npy')\n",
    "train_files = shuffle(train_files, random_state=3101)\n",
    "len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator():\n",
    "    for file in train_files:\n",
    "        dataset = np.load(file)\n",
    "        target= dataset[:,:,-1].reshape(120,120,1)\n",
    "        remove_minus = np.where(target < 0, 0, target)\n",
    "        feature = dataset[:,:,:4]\n",
    "\n",
    "        yield (feature, remove_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_TRAIN = int(len(train_files)*.7)\n",
    "\n",
    "# full_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([120,120,4]),tf.TensorShape([120,120,1])))\n",
    "\n",
    "# train_dataset = full_dataset.take(NUM_TRAIN)\n",
    "# train_dataset = train_dataset.batch(64).prefetch(1).repeat()\n",
    "# val_dataset = full_dataset.skip(NUM_TRAIN)\n",
    "# val_dataset = val_dataset.batch(64).prefetch(1).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = int(len(train_files)*.7)\n",
    "\n",
    "full_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([120,120,4]),tf.TensorShape([120,120,1])))\n",
    "\n",
    "train_dataset = full_dataset.take(NUM_TRAIN)\n",
    "train_dataset = train_dataset.batch(64).prefetch(1)\n",
    "val_dataset = full_dataset.skip(NUM_TRAIN)\n",
    "val_dataset = val_dataset.batch(64).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitConvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super(InitConvBlock, self).__init__()\n",
    "        self.f = filters\n",
    "        self.k = kernel_size\n",
    "        \n",
    "        self.downsampling = layers.Conv2D(self.f, (1, 1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        self.downbatch = layers.BatchNormalization()\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(self.f, self.k, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        self.batch1 = layers.BatchNormalization()\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv2 = layers.Conv2D(self.f, self.k, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        \n",
    "    def call(self, inp, TRAINING):\n",
    "\n",
    "        shortcut = self.downbatch(self.downsampling(inp), training=TRAINING)\n",
    "\n",
    "        inp = self.conv2(self.activation1(self.batch1(self.conv1(inp), training=TRAINING)))\n",
    "        inp = layers.add([shortcut, inp])\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.f = filters\n",
    "        self.k = kernel_size\n",
    "        \n",
    "        self.downsampling = layers.Conv2D(self.f, (1, 1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        self.downbatch = layers.BatchNormalization()\n",
    "        \n",
    "        self.batch1 = layers.BatchNormalization()\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv1 = layers.Conv2D(self.f/4, (1, 1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        \n",
    "        self.batch2 = layers.BatchNormalization()\n",
    "        self.activation2 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv2 = layers.Conv2D(self.f/4, self.k, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        \n",
    "        self.outconv = layers.Conv2D(self.f, (1, 1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        self.outbatch = layers.BatchNormalization()\n",
    "        self.outact = layers.Activation(tf.nn.leaky_relu)\n",
    "        \n",
    "    def call(self, inp, TRAINING):\n",
    "        \n",
    "        shortcut = self.downbatch(self.downsampling(inp), training=TRAINING)\n",
    "        \n",
    "        inp = self.conv1(self.activation1(self.batch1(inp, training=TRAINING)))\n",
    "        inp = self.conv2(self.activation2(self.batch2(inp, training=TRAINING)))\n",
    "        inp = self.outbatch(self.outconv(inp), training=TRAINING)\n",
    "        inp = self.outact(layers.add([shortcut, inp]))\n",
    "                \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.f = filters\n",
    "        self.k = kernel_size\n",
    "        self.s = strides\n",
    "        \n",
    "        self.deconv1 = layers.Conv2DTranspose(self.f, self.k, kernel_initializer='he_normal', strides=self.s, padding='same')\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        \n",
    "    def call(self, inp):\n",
    "        \n",
    "        inp = self.activation1(self.deconv1(inp))\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(Model):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n = [64, 128, 256, 512, 1024] # number of nodes\n",
    "        self.k = (3, 3) # kernal size\n",
    "        self.s = (2, 2) # stride (= pooling size)\n",
    "        \n",
    "        \n",
    "        self.conv_block1 = InitConvBlock(self.n[0], self.k)\n",
    "        self.conv_block1_2 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block1_3 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block1_4 = ConvBlock(self.n[0], self.k)\n",
    "        self.pool1 = layers.Conv2D(self.n[1], self.s, self.s)\n",
    "        \n",
    "        self.conv_block2 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block2_2 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block2_3 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block2_4 = ConvBlock(self.n[1], self.k)\n",
    "        self.pool2 = layers.Conv2D(self.n[2], self.s, self.s)\n",
    "        \n",
    "        self.conv_block3 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block3_2 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block3_3 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block3_4 = ConvBlock(self.n[2], self.k)\n",
    "        self.pool3 = layers.Conv2D(self.n[3], self.s, self.s)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_2 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_3 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_4 = ConvBlock(self.n[3], self.k)\n",
    "        self.pool4 = layers.Conv2D(self.n[4], self.s, self.s)\n",
    "        \n",
    "        self.conv_bottom = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_2 = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_3 = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_4 = ConvBlock(self.n[4], self.k)\n",
    "        \n",
    "        self.deconv_block1 = layers.UpSampling2D()\n",
    "        self.conv_block_r1 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_2 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_3 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_4 = ConvBlock(self.n[3], self.k)\n",
    "        \n",
    "        self.deconv_block2 = layers.UpSampling2D()\n",
    "        self.conv_block_r2 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block_r2_2 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block_r2_3 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block_r2_4 = ConvBlock(self.n[2], self.k)\n",
    "        \n",
    "        self.deconv_block3 = layers.UpSampling2D()\n",
    "        self.conv_block_r3 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block_r3_2 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block_r3_3 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block_r3_4 = ConvBlock(self.n[1], self.k)\n",
    "        \n",
    "        self.deconv_block4 = layers.UpSampling2D()\n",
    "        self.conv_block_r4 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block_r4_2 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block_r4_3 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block_r4_4 = ConvBlock(self.n[0], self.k)\n",
    "        \n",
    "        self.padding = layers.ZeroPadding2D(((1, 0), (0, 1)))\n",
    "        self.output_conv = layers.Conv2D(1, (1, 1), activation='sigmoid')\n",
    "        \n",
    "    def call(self, inp, TRAINING=True):\n",
    "        inp = inp/255\n",
    "        \n",
    "        conv1 = self.conv_block1(inp, TRAINING)\n",
    "        conv1 = self.conv_block1_2(conv1, TRAINING)\n",
    "        conv1 = self.conv_block1_3(conv1, TRAINING)\n",
    "        conv1 = self.conv_block1_4(conv1, TRAINING)\n",
    "        pooled1 = self.pool1(conv1)\n",
    "        \n",
    "        conv2 = self.conv_block2(pooled1, TRAINING)\n",
    "        conv2 = self.conv_block2_2(conv2, TRAINING)\n",
    "        conv2 = self.conv_block2_3(conv2, TRAINING)\n",
    "        conv2 = self.conv_block2_4(conv2, TRAINING)\n",
    "        pooled2 = self.pool2(conv2)\n",
    "        \n",
    "        conv3 = self.conv_block3(pooled2, TRAINING)\n",
    "        conv3 = self.conv_block3_2(conv3, TRAINING)\n",
    "        conv3 = self.conv_block3_3(conv3, TRAINING)\n",
    "        conv3 = self.conv_block3_4(conv3, TRAINING)\n",
    "        pooled3 = self.pool3(conv3)\n",
    "        \n",
    "        conv4 = self.conv_block4(pooled3, TRAINING)\n",
    "        conv4 = self.conv_block4_2(conv4, TRAINING)\n",
    "        conv4 = self.conv_block4_3(conv4, TRAINING)\n",
    "        conv4 = self.conv_block4_4(conv4, TRAINING)\n",
    "        pooled4 = self.pool4(conv4)\n",
    "        \n",
    "        bottom = self.conv_bottom(pooled4, TRAINING)\n",
    "        bottom = self.conv_bottom_2(bottom, TRAINING)\n",
    "        bottom = self.conv_bottom_3(bottom, TRAINING)\n",
    "        bottom = self.conv_bottom_4(bottom, TRAINING)\n",
    "        \n",
    "        deconv1 = self.padding(self.deconv_block1(bottom))\n",
    "        deconv1 = layers.concatenate([deconv1, conv4])\n",
    "        deconv1 = self.conv_block_r1(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_2(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_3(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_4(deconv1, TRAINING)\n",
    "        \n",
    "        deconv2 = self.deconv_block2(deconv1)\n",
    "        deconv2 = layers.concatenate([deconv2, conv3])\n",
    "        deconv2 = self.conv_block_r2(deconv2, TRAINING)\n",
    "        deconv2 = self.conv_block_r2_2(deconv2, TRAINING)\n",
    "        deconv2 = self.conv_block_r2_3(deconv2, TRAINING)\n",
    "        deconv2 = self.conv_block_r2_4(deconv2, TRAINING)\n",
    "        \n",
    "        deconv3 = self.deconv_block3(deconv2)\n",
    "        deconv3 = layers.concatenate([deconv3, conv2])\n",
    "        deconv3 = self.conv_block_r3(deconv3, TRAINING)\n",
    "        deconv3 = self.conv_block_r3_2(deconv3, TRAINING)\n",
    "        deconv3 = self.conv_block_r3_3(deconv3, TRAINING)\n",
    "        deconv3 = self.conv_block_r3_4(deconv3, TRAINING)\n",
    "        \n",
    "        deconv4 = self.deconv_block4(deconv3)\n",
    "        deconv4 = layers.concatenate([deconv4, conv1])\n",
    "        deconv4 = self.conv_block_r4(deconv4, TRAINING)\n",
    "        deconv4 = self.conv_block_r4_2(deconv4, TRAINING)\n",
    "        deconv4 = self.conv_block_r4_3(deconv4, TRAINING)\n",
    "        deconv4 = self.conv_block_r4_4(deconv4, TRAINING)\n",
    "        \n",
    "        return self.output_conv(deconv4)*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(Model):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n = [64, 128, 256, 512, 1024] # number of nodes\n",
    "        self.k = (3, 3) # kernal size\n",
    "        self.s = (2, 2) # stride (= pooling size)\n",
    "        \n",
    "        \n",
    "        self.conv_block1 = InitConvBlock(self.n[0], self.k)\n",
    "        self.conv_block1_2 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block1_3 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block1_4 = ConvBlock(self.n[0], self.k)\n",
    "        self.pool1 = layers.Conv2D(self.n[1], self.s, self.s)\n",
    "        \n",
    "        self.conv_block2 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block2_2 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block2_3 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block2_4 = ConvBlock(self.n[1], self.k)\n",
    "        self.pool2 = layers.Conv2D(self.n[2], self.s, self.s)\n",
    "        \n",
    "        self.conv_block3 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block3_2 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block3_3 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block3_4 = ConvBlock(self.n[2], self.k)\n",
    "        self.pool3 = layers.Conv2D(self.n[3], self.s, self.s)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_2 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_3 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_4 = ConvBlock(self.n[3], self.k)\n",
    "        self.pool4 = layers.Conv2D(self.n[4], self.s, self.s)\n",
    "        \n",
    "        self.conv_bottom = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_2 = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_3 = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_4 = ConvBlock(self.n[4], self.k)\n",
    "        \n",
    "        self.deconv_block1 = layers.UpSampling2D()\n",
    "        self.conv_block_r1 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_2 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_3 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_4 = ConvBlock(self.n[3], self.k)\n",
    "        \n",
    "        self.deconv_block2 = layers.UpSampling2D()\n",
    "        self.conv_block_r2 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block_r2_2 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block_r2_3 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block_r2_4 = ConvBlock(self.n[2], self.k)\n",
    "        \n",
    "        self.deconv_block3 = layers.UpSampling2D()\n",
    "        self.conv_block_r3 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block_r3_2 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block_r3_3 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block_r3_4 = ConvBlock(self.n[1], self.k)\n",
    "        \n",
    "        self.deconv_block4 = layers.UpSampling2D()\n",
    "        self.conv_block_r4 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block_r4_2 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block_r4_3 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block_r4_4 = ConvBlock(self.n[0], self.k)\n",
    "        \n",
    "        self.padding = layers.ZeroPadding2D(((1, 0), (0, 1)))\n",
    "        self.output_conv = layers.Conv2D(1, (1, 1), activation='sigmoid')\n",
    "        \n",
    "    def call(self, inp, TRAINING=True):\n",
    "        inp = inp/255\n",
    "        \n",
    "        conv1 = self.conv_block1(inp, TRAINING)\n",
    "        conv1 = self.conv_block1_2(conv1, TRAINING)\n",
    "        conv1 = self.conv_block1_3(conv1, TRAINING)\n",
    "        conv1 = self.conv_block1_4(conv1, TRAINING)\n",
    "        pooled1 = self.pool1(conv1)\n",
    "        \n",
    "        conv2 = self.conv_block2(pooled1, TRAINING)\n",
    "        conv2 = self.conv_block2_2(conv2, TRAINING)\n",
    "        conv2 = self.conv_block2_3(conv2, TRAINING)\n",
    "        conv2 = self.conv_block2_4(conv2, TRAINING)\n",
    "        pooled2 = self.pool2(conv2)\n",
    "        \n",
    "        conv3 = self.conv_block3(pooled2, TRAINING)\n",
    "        conv3 = self.conv_block3_2(conv3, TRAINING)\n",
    "        conv3 = self.conv_block3_3(conv3, TRAINING)\n",
    "        conv3 = self.conv_block3_4(conv3, TRAINING)\n",
    "        pooled3 = self.pool3(conv3)\n",
    "        \n",
    "        conv4 = self.conv_block4(pooled3, TRAINING)\n",
    "        conv4 = self.conv_block4_2(conv4, TRAINING)\n",
    "        conv4 = self.conv_block4_3(conv4, TRAINING)\n",
    "        conv4 = self.conv_block4_4(conv4, TRAINING)\n",
    "        pooled4 = self.pool4(conv4)\n",
    "        \n",
    "        bottom = self.conv_bottom(pooled4, TRAINING)\n",
    "        bottom = self.conv_bottom_2(bottom, TRAINING)\n",
    "        bottom = self.conv_bottom_3(bottom, TRAINING)\n",
    "        bottom = self.conv_bottom_4(bottom, TRAINING)\n",
    "        \n",
    "        deconv1 = self.padding(self.deconv_block1(bottom))\n",
    "        deconv1 = layers.concatenate([deconv1, conv4])\n",
    "        deconv1 = self.conv_block_r1(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_2(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_3(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_4(deconv1, TRAINING)\n",
    "        \n",
    "        deconv2 = self.deconv_block2(deconv1)\n",
    "        deconv2 = layers.concatenate([deconv2, conv3])\n",
    "        deconv2 = self.conv_block_r2(deconv2, TRAINING)\n",
    "        deconv2 = self.conv_block_r2_2(deconv2, TRAINING)\n",
    "        deconv2 = self.conv_block_r2_3(deconv2, TRAINING)\n",
    "        deconv2 = self.conv_block_r2_4(deconv2, TRAINING)\n",
    "        \n",
    "        deconv3 = self.deconv_block3(deconv2)\n",
    "        deconv3 = layers.concatenate([deconv3, conv2])\n",
    "        deconv3 = self.conv_block_r3(deconv3, TRAINING)\n",
    "        deconv3 = self.conv_block_r3_2(deconv3, TRAINING)\n",
    "        deconv3 = self.conv_block_r3_3(deconv3, TRAINING)\n",
    "        deconv3 = self.conv_block_r3_4(deconv3, TRAINING)\n",
    "        \n",
    "        deconv4 = self.deconv_block4(deconv3)\n",
    "        deconv4 = layers.concatenate([deconv4, conv1])\n",
    "        deconv4 = self.conv_block_r4(deconv4, TRAINING)\n",
    "        deconv4 = self.conv_block_r4_2(deconv4, TRAINING)\n",
    "        deconv4 = self.conv_block_r4_3(deconv4, TRAINING)\n",
    "        deconv4 = self.conv_block_r4_4(deconv4, TRAINING)\n",
    "        \n",
    "        return self.output_conv(deconv4)*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='mae', optimizer=opt)\n",
    "# model.fit(train_dataset, epochs = 200,\n",
    "#           callbacks = callbacks, validation_data=val_dataset,\n",
    "#           steps_per_epoch = 100, validation_steps=30,\n",
    "#           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_schedule = optimizers.schedules.ExponentialDecay(LEARNING_RATE, DECAY_STEPS, DECAY_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    with strategy.scope():\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath='./models/DeepResUNet_%d.h5' % i,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss',\n",
    "                verbose=1),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,  patience=20)\n",
    "        ]\n",
    "        lr_schedule = optimizers.schedules.InverseTimeDecay(LEARNING_RATE, DECAY_STEPS, DECAY_RATE, staircase=True)\n",
    "        model = UNet()\n",
    "\n",
    "        opt = tf.optimizers.Adam(learning_rate=lr_schedule, epsilon=EPSILON)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "        model.fit(train_dataset, epochs = 200,\n",
    "                  callbacks = callbacks, validation_data=val_dataset,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: 2.76247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "model.load_weights('./models/DeepResUNet_%d.h5' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './data/test'\n",
    "test_files = sorted(glob.glob(test_path + '/*.npy'))\n",
    "\n",
    "X_test = []\n",
    "\n",
    "for file in tqdm(test_files, desc = 'test'):\n",
    "    data = np.load(file)\n",
    "    X_test.append(data)\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    pred = model.predict(X_test.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:,1:] = pred.reshape(-1, 14400).astype(int)\n",
    "submission.to_csv('./results/Dacon_DeepResUNet_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
