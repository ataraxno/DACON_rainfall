{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "import datetime\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.003\n",
    "DECAY_STEPS = 1000\n",
    "DECAY_RATE = 2\n",
    "EPSILON = 1e-06\n",
    "BEST_PATH = './models/DeepResUNet.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62735"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = glob.glob('./data/train/*.npy')\n",
    "train_files = shuffle(train_files, random_state=3101)\n",
    "len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator():\n",
    "    for file in train_files:\n",
    "        dataset = np.load(file)\n",
    "        target= dataset[:,:,-1].reshape(120,120,1)\n",
    "        remove_minus = np.where(target < 0, 0, target)\n",
    "        feature = dataset[:,:,:4]\n",
    "\n",
    "        yield (feature, remove_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_TRAIN = int(len(train_files)*.7)\n",
    "\n",
    "# full_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([120,120,4]),tf.TensorShape([120,120,1])))\n",
    "\n",
    "# train_dataset = full_dataset.take(NUM_TRAIN)\n",
    "# train_dataset = train_dataset.batch(64).prefetch(1).repeat()\n",
    "# val_dataset = full_dataset.skip(NUM_TRAIN)\n",
    "# val_dataset = val_dataset.batch(64).prefetch(1).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = int(len(train_files)*.7)\n",
    "\n",
    "full_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([120,120,4]),tf.TensorShape([120,120,1])))\n",
    "\n",
    "train_dataset = full_dataset.take(NUM_TRAIN)\n",
    "train_dataset = train_dataset.batch(64).prefetch(1)\n",
    "val_dataset = full_dataset.skip(NUM_TRAIN)\n",
    "val_dataset = val_dataset.batch(64).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitConvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super(InitConvBlock, self).__init__()\n",
    "        self.f = filters\n",
    "        self.k = kernel_size\n",
    "        \n",
    "        self.downsampling = layers.Conv2D(self.f, (1, 1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        self.downbatch = layers.BatchNormalization()\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(self.f, self.k, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        self.batch1 = layers.BatchNormalization()\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv2 = layers.Conv2D(self.f, self.k, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        \n",
    "    def call(self, inp, TRAINING):\n",
    "\n",
    "        shortcut = self.downbatch(self.downsampling(inp), training=TRAINING)\n",
    "\n",
    "        inp = self.conv2(self.activation1(self.batch1(self.conv1(inp), training=TRAINING)))\n",
    "        inp = layers.add([shortcut, inp])\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.f = filters\n",
    "        self.k = kernel_size\n",
    "        \n",
    "        self.downsampling = layers.Conv2D(self.f, (1, 1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        self.downbatch = layers.BatchNormalization()\n",
    "        \n",
    "        self.batch1 = layers.BatchNormalization()\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv1 = layers.Conv2D(self.f/4, (1, 1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        \n",
    "        self.batch2 = layers.BatchNormalization()\n",
    "        self.activation2 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv2 = layers.Conv2D(self.f/4, self.k, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        \n",
    "        self.outconv = layers.Conv2D(self.f, (1, 1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        self.outbatch = layers.BatchNormalization()\n",
    "        self.outact = layers.Activation(tf.nn.leaky_relu)\n",
    "        \n",
    "    def call(self, inp, TRAINING):\n",
    "        \n",
    "        shortcut = self.downbatch(self.downsampling(inp), training=TRAINING)\n",
    "        \n",
    "        inp = self.conv1(self.activation1(self.batch1(inp, training=TRAINING)))\n",
    "        inp = self.conv2(self.activation2(self.batch2(inp, training=TRAINING)))\n",
    "        inp = self.outbatch(self.outconv(inp), training=TRAINING)\n",
    "        inp = self.outact(layers.add([shortcut, inp]))\n",
    "                \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.f = filters\n",
    "        self.k = kernel_size\n",
    "        self.s = strides\n",
    "        \n",
    "        self.deconv1 = layers.Conv2DTranspose(self.f, self.k, kernel_initializer='he_normal', strides=self.s, padding='same')\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        \n",
    "    def call(self, inp):\n",
    "        \n",
    "        inp = self.activation1(self.deconv1(inp))\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(Model):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n = [64, 128, 256, 512, 1024] # number of nodes\n",
    "        self.k = (3, 3) # kernal size\n",
    "        self.s = (2, 2) # stride (= pooling size)\n",
    "        \n",
    "        \n",
    "        self.conv_block1 = InitConvBlock(self.n[0], self.k)\n",
    "        self.conv_block1_2 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block1_3 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block1_4 = ConvBlock(self.n[0], self.k)\n",
    "        self.pool1 = layers.Conv2D(self.n[1], self.s, self.s)\n",
    "        \n",
    "        self.conv_block2 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block2_2 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block2_3 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block2_4 = ConvBlock(self.n[1], self.k)\n",
    "        self.pool2 = layers.Conv2D(self.n[2], self.s, self.s)\n",
    "        \n",
    "        self.conv_block3 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block3_2 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block3_3 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block3_4 = ConvBlock(self.n[2], self.k)\n",
    "        self.pool3 = layers.Conv2D(self.n[3], self.s, self.s)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_2 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_3 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_4 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_5 = ConvBlock(self.n[3], self.k)\n",
    "        self.pool4 = layers.Conv2D(self.n[4], self.s, self.s)\n",
    "        \n",
    "        self.conv_bottom = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_2 = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_3 = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_4 = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_5 = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_6 = ConvBlock(self.n[4], self.k)\n",
    "        \n",
    "        self.deconv_block1 = layers.UpSampling2D()\n",
    "        self.conv_block_r1 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_2 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_3 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_4 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_5 = ConvBlock(self.n[3], self.k)\n",
    "        \n",
    "        self.deconv_block2 = layers.UpSampling2D()\n",
    "        self.conv_block_r2 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block_r2_2 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block_r2_3 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block_r2_4 = ConvBlock(self.n[2], self.k)\n",
    "        \n",
    "        self.deconv_block3 = layers.UpSampling2D()\n",
    "        self.conv_block_r3 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block_r3_2 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block_r3_3 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block_r3_4 = ConvBlock(self.n[1], self.k)\n",
    "        \n",
    "        self.deconv_block4 = layers.UpSampling2D()\n",
    "        self.conv_block_r4 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block_r4_2 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block_r4_3 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block_r4_4 = ConvBlock(self.n[0], self.k)\n",
    "        \n",
    "        self.padding = layers.ZeroPadding2D(((1, 0), (0, 1)))\n",
    "        self.output_conv = layers.Conv2D(1, (1, 1), activation='sigmoid')\n",
    "        \n",
    "    def call(self, inp, TRAINING=True):\n",
    "        inp = inp/255\n",
    "        \n",
    "        conv1 = self.conv_block1(inp, TRAINING)\n",
    "        conv1 = self.conv_block1_2(conv1, TRAINING)\n",
    "        conv1 = self.conv_block1_3(conv1, TRAINING)\n",
    "        conv1 = self.conv_block1_4(conv1, TRAINING)\n",
    "        pooled1 = self.pool1(conv1)\n",
    "        \n",
    "        conv2 = self.conv_block2(pooled1, TRAINING)\n",
    "        conv2 = self.conv_block2_2(conv2, TRAINING)\n",
    "        conv2 = self.conv_block2_3(conv2, TRAINING)\n",
    "        conv2 = self.conv_block2_4(conv2, TRAINING)\n",
    "        pooled2 = self.pool2(conv2)\n",
    "        \n",
    "        conv3 = self.conv_block3(pooled2, TRAINING)\n",
    "        conv3 = self.conv_block3_2(conv3, TRAINING)\n",
    "        conv3 = self.conv_block3_3(conv3, TRAINING)\n",
    "        conv3 = self.conv_block3_4(conv3, TRAINING)\n",
    "        pooled3 = self.pool3(conv3)\n",
    "        \n",
    "        conv4 = self.conv_block4(pooled3, TRAINING)\n",
    "        conv4 = self.conv_block4_2(conv4, TRAINING)\n",
    "        conv4 = self.conv_block4_3(conv4, TRAINING)\n",
    "        conv4 = self.conv_block4_4(conv4, TRAINING)\n",
    "        conv4 = self.conv_block4_5(conv4, TRAINING)\n",
    "        pooled4 = self.pool4(conv4)\n",
    "        \n",
    "        bottom = self.conv_bottom(pooled4, TRAINING)\n",
    "        bottom = self.conv_bottom_2(bottom, TRAINING)\n",
    "        bottom = self.conv_bottom_3(bottom, TRAINING)\n",
    "        bottom = self.conv_bottom_4(bottom, TRAINING)\n",
    "        bottom = self.conv_bottom_5(bottom, TRAINING)\n",
    "        bottom = self.conv_bottom_6(bottom, TRAINING)\n",
    "        \n",
    "        deconv1 = self.padding(self.deconv_block1(bottom))\n",
    "        deconv1 = layers.concatenate([deconv1, conv4])\n",
    "        deconv1 = self.conv_block_r1(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_2(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_3(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_4(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_5(deconv1, TRAINING)\n",
    "        \n",
    "        deconv2 = self.deconv_block2(deconv1)\n",
    "        deconv2 = layers.concatenate([deconv2, conv3])\n",
    "        deconv2 = self.conv_block_r2(deconv2, TRAINING)\n",
    "        deconv2 = self.conv_block_r2_2(deconv2, TRAINING)\n",
    "        deconv2 = self.conv_block_r2_3(deconv2, TRAINING)\n",
    "        deconv2 = self.conv_block_r2_4(deconv2, TRAINING)\n",
    "        \n",
    "        deconv3 = self.deconv_block3(deconv2)\n",
    "        deconv3 = layers.concatenate([deconv3, conv2])\n",
    "        deconv3 = self.conv_block_r3(deconv3, TRAINING)\n",
    "        deconv3 = self.conv_block_r3_2(deconv3, TRAINING)\n",
    "        deconv3 = self.conv_block_r3_3(deconv3, TRAINING)\n",
    "        deconv3 = self.conv_block_r3_4(deconv3, TRAINING)\n",
    "        \n",
    "        deconv4 = self.deconv_block4(deconv3)\n",
    "        deconv4 = layers.concatenate([deconv4, conv1])\n",
    "        deconv4 = self.conv_block_r4(deconv4, TRAINING)\n",
    "        deconv4 = self.conv_block_r4_2(deconv4, TRAINING)\n",
    "        deconv4 = self.conv_block_r4_3(deconv4, TRAINING)\n",
    "        deconv4 = self.conv_block_r4_4(deconv4, TRAINING)\n",
    "        \n",
    "        return self.output_conv(deconv4)*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='mae', optimizer=opt)\n",
    "# model.fit(train_dataset, epochs = 200,\n",
    "#           callbacks = callbacks, validation_data=val_dataset,\n",
    "#           steps_per_epoch = 100, validation_steps=30,\n",
    "#           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_schedule = optimizers.schedules.ExponentialDecay(LEARNING_RATE, DECAY_STEPS, DECAY_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 2674/2674 [00:00<00:00, 3207.47it/s]\n"
     ]
    }
   ],
   "source": [
    "test_path = './data/test'\n",
    "test_files = sorted(glob.glob(test_path + '/*.npy'))\n",
    "\n",
    "X_test = []\n",
    "\n",
    "for file in tqdm(test_files, desc = 'test'):\n",
    "    data = np.load(file)\n",
    "    X_test.append(data)\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/200\n",
      "INFO:tensorflow:batch_all_reduce: 644 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 644 all-reduces with algorithm = nccl, num_packs = 1\n",
      "      1/Unknown - 0s 71us/step - loss: 2.9708WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "      2/Unknown - 1s 667ms/step - loss: 7.3878WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.4847s vs `on_train_batch_end` time: 0.8488s). Check your callbacks.\n",
      "    687/Unknown - 317s 462ms/step - loss: 3.7217\n",
      "Epoch 00001: val_loss improved from inf to 3.66246, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 434s 631ms/step - loss: 3.7217 - val_loss: 3.6625\n",
      "Epoch 2/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 3.3365\n",
      "Epoch 00002: val_loss improved from 3.66246 to 3.25716, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 404s 589ms/step - loss: 3.3365 - val_loss: 3.2572\n",
      "Epoch 3/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 3.1844\n",
      "Epoch 00003: val_loss improved from 3.25716 to 3.16838, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 403s 587ms/step - loss: 3.1844 - val_loss: 3.1684\n",
      "Epoch 4/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 3.1179\n",
      "Epoch 00004: val_loss improved from 3.16838 to 3.14876, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 403s 587ms/step - loss: 3.1179 - val_loss: 3.1488\n",
      "Epoch 5/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 3.0831\n",
      "Epoch 00005: val_loss improved from 3.14876 to 3.10495, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 403s 587ms/step - loss: 3.0831 - val_loss: 3.1050\n",
      "Epoch 6/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 3.0579\n",
      "Epoch 00006: val_loss improved from 3.10495 to 3.06074, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 403s 587ms/step - loss: 3.0579 - val_loss: 3.0607\n",
      "Epoch 7/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 3.0324\n",
      "Epoch 00007: val_loss improved from 3.06074 to 3.05410, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 402s 585ms/step - loss: 3.0324 - val_loss: 3.0541\n",
      "Epoch 8/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 3.0154\n",
      "Epoch 00008: val_loss improved from 3.05410 to 3.02871, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 402s 585ms/step - loss: 3.0154 - val_loss: 3.0287\n",
      "Epoch 9/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 3.0030\n",
      "Epoch 00009: val_loss improved from 3.02871 to 3.00994, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 401s 583ms/step - loss: 3.0030 - val_loss: 3.0099\n",
      "Epoch 10/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9893\n",
      "Epoch 00010: val_loss improved from 3.00994 to 3.00680, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 402s 586ms/step - loss: 2.9893 - val_loss: 3.0068\n",
      "Epoch 11/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9782\n",
      "Epoch 00011: val_loss improved from 3.00680 to 2.99793, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 402s 585ms/step - loss: 2.9782 - val_loss: 2.9979\n",
      "Epoch 12/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9701\n",
      "Epoch 00012: val_loss improved from 2.99793 to 2.99513, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 402s 585ms/step - loss: 2.9701 - val_loss: 2.9951\n",
      "Epoch 13/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9615\n",
      "Epoch 00013: val_loss improved from 2.99513 to 2.99181, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 405s 590ms/step - loss: 2.9615 - val_loss: 2.9918\n",
      "Epoch 14/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9525\n",
      "Epoch 00014: val_loss improved from 2.99181 to 2.98061, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 402s 585ms/step - loss: 2.9525 - val_loss: 2.9806\n",
      "Epoch 15/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9461\n",
      "Epoch 00015: val_loss improved from 2.98061 to 2.97405, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 401s 583ms/step - loss: 2.9461 - val_loss: 2.9741\n",
      "Epoch 16/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9399\n",
      "Epoch 00016: val_loss improved from 2.97405 to 2.96884, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 400s 582ms/step - loss: 2.9399 - val_loss: 2.9688\n",
      "Epoch 17/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9343\n",
      "Epoch 00017: val_loss improved from 2.96884 to 2.96453, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 401s 584ms/step - loss: 2.9343 - val_loss: 2.9645\n",
      "Epoch 18/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9293\n",
      "Epoch 00018: val_loss improved from 2.96453 to 2.95449, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 401s 583ms/step - loss: 2.9293 - val_loss: 2.9545\n",
      "Epoch 19/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9242\n",
      "Epoch 00019: val_loss did not improve from 2.95449\n",
      "687/687 [==============================] - 400s 583ms/step - loss: 2.9242 - val_loss: 2.9589\n",
      "Epoch 20/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9196\n",
      "Epoch 00020: val_loss improved from 2.95449 to 2.94533, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 401s 584ms/step - loss: 2.9196 - val_loss: 2.9453\n",
      "Epoch 21/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9150\n",
      "Epoch 00021: val_loss did not improve from 2.94533\n",
      "687/687 [==============================] - 399s 581ms/step - loss: 2.9150 - val_loss: 2.9505\n",
      "Epoch 22/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9104\n",
      "Epoch 00022: val_loss did not improve from 2.94533\n",
      "687/687 [==============================] - 399s 581ms/step - loss: 2.9104 - val_loss: 2.9532\n",
      "Epoch 23/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9071\n",
      "Epoch 00023: val_loss improved from 2.94533 to 2.94370, saving model to ./models/DeepResUNet_0.h5\n",
      "687/687 [==============================] - 402s 585ms/step - loss: 2.9071 - val_loss: 2.9437\n",
      "Epoch 24/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.9026\n",
      "Epoch 00024: val_loss did not improve from 2.94370\n",
      "687/687 [==============================] - 399s 581ms/step - loss: 2.9026 - val_loss: 2.9471\n",
      "Epoch 25/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.8992\n",
      "Epoch 00025: val_loss did not improve from 2.94370\n",
      "687/687 [==============================] - 400s 583ms/step - loss: 2.8992 - val_loss: 2.9461\n",
      "Epoch 26/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.8956\n",
      "Epoch 00026: val_loss did not improve from 2.94370\n",
      "687/687 [==============================] - 402s 585ms/step - loss: 2.8956 - val_loss: 2.9447\n",
      "Epoch 27/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.8924\n",
      "Epoch 00027: val_loss did not improve from 2.94370\n",
      "687/687 [==============================] - 401s 584ms/step - loss: 2.8924 - val_loss: 2.9443\n",
      "Epoch 28/200\n",
      "687/687 [==============================] - ETA: 0s - loss: 2.8895"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    with strategy.scope():\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath='./models/DeepResUNet_%d.h5' % i,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss',\n",
    "                verbose=1),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,  patience=20),\n",
    "            tensorboard_callback\n",
    "        ]\n",
    "        lr_schedule = optimizers.schedules.InverseTimeDecay(LEARNING_RATE, DECAY_STEPS, DECAY_RATE, staircase=True)\n",
    "        model = UNet()\n",
    "        opt = tf.optimizers.Adam(learning_rate=lr_schedule, epsilon=EPSILON)\n",
    "        model.compile(loss='mae', optimizer=opt)\n",
    "        model.predict(X_test.astype('float32')[:2, ...])\n",
    "        model.load_weights('./models/DeepResUNet_%d.h5' % i)\n",
    "        model.fit(train_dataset, epochs = 200,\n",
    "                  callbacks = callbacks, validation_data=val_dataset,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: 2.76247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in range(5):\n",
    "    model.load_weights('./models/DeepResUNet_%d.h5' % i)\n",
    "    pred.append(model.predict(X_test.astype('float32')))\n",
    "pred = np.array(pred).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:,1:] = pred.reshape(-1, 14400).astype(int)\n",
    "submission.to_csv('./results/Dacon_DeepResUNet_2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
