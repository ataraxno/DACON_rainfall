{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "import datetime\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.02\n",
    "DECAY_STEPS = 250 # 1000 -> 250\n",
    "DECAY_RATE = 2\n",
    "EPSILON = 1e-06\n",
    "BEST_PATH = './models/DeepResUNet_dif.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62735"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = glob.glob('./data/train/*.npy')\n",
    "train_files = shuffle(train_files, random_state=3101)\n",
    "len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator():\n",
    "    for file in train_files:\n",
    "        dataset = np.load(file)\n",
    "        target= dataset[:,:,-1].reshape(120,120,1)\n",
    "        remove_minus = np.where(target < 0, 0, target)\n",
    "        feature = dataset[:,:,:4]\n",
    "\n",
    "        yield (feature, remove_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_TRAIN = int(len(train_files)*.7)\n",
    "\n",
    "# full_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([120,120,4]),tf.TensorShape([120,120,1])))\n",
    "\n",
    "# train_dataset = full_dataset.take(NUM_TRAIN)\n",
    "# train_dataset = train_dataset.batch(64).prefetch(1).repeat()\n",
    "# val_dataset = full_dataset.skip(NUM_TRAIN)\n",
    "# val_dataset = val_dataset.batch(64).prefetch(1).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = int(len(train_files)*.7)\n",
    "\n",
    "full_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([120,120,4]),tf.TensorShape([120,120,1])))\n",
    "\n",
    "train_dataset = full_dataset.take(NUM_TRAIN)\n",
    "train_dataset = train_dataset.batch(64).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = full_dataset.skip(NUM_TRAIN)\n",
    "val_dataset = val_dataset.batch(64).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitConvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super(InitConvBlock, self).__init__()\n",
    "        self.f = filters\n",
    "        self.k = kernel_size\n",
    "        \n",
    "        self.downsampling = layers.Conv2D(self.f, (1, 1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        self.downbatch = layers.BatchNormalization()\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(self.f, self.k, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        self.batch1 = layers.BatchNormalization()\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv2 = layers.Conv2D(self.f, self.k, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        \n",
    "    def call(self, inp, TRAINING):\n",
    "\n",
    "        shortcut = self.downbatch(self.downsampling(inp), training=TRAINING)\n",
    "\n",
    "        inp = self.conv2(self.activation1(self.batch1(self.conv1(inp), training=TRAINING)))\n",
    "        inp = layers.add([shortcut, inp])\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.f = filters\n",
    "        self.k = kernel_size\n",
    "        \n",
    "        self.downsampling = layers.Conv2D(self.f, (1, 1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        self.downbatch = layers.BatchNormalization()\n",
    "        \n",
    "        self.batch1 = layers.BatchNormalization()\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv1 = layers.Conv2D(self.f/4, (1, 1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        \n",
    "        self.batch2 = layers.BatchNormalization()\n",
    "        self.activation2 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv2 = layers.Conv2D(self.f/4, self.k, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        \n",
    "        self.outconv = layers.Conv2D(self.f, (1, 1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1e-4), padding='same')\n",
    "        self.outbatch = layers.BatchNormalization()\n",
    "        self.outact = layers.Activation(tf.nn.leaky_relu)\n",
    "        \n",
    "    def call(self, inp, TRAINING):\n",
    "        \n",
    "        shortcut = self.downbatch(self.downsampling(inp), training=TRAINING)\n",
    "        \n",
    "        inp = self.conv1(self.activation1(self.batch1(inp, training=TRAINING)))\n",
    "        inp = self.conv2(self.activation2(self.batch2(inp, training=TRAINING)))\n",
    "        inp = self.outbatch(self.outconv(inp), training=TRAINING)\n",
    "        inp = self.outact(layers.add([shortcut, inp]))\n",
    "                \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.f = filters\n",
    "        self.k = kernel_size\n",
    "        self.s = strides\n",
    "        \n",
    "        self.deconv1 = layers.Conv2DTranspose(self.f, self.k, kernel_initializer='he_normal', strides=self.s, padding='same')\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        \n",
    "    def call(self, inp):\n",
    "        \n",
    "        inp = self.activation1(self.deconv1(inp))\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(Model):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n = [64, 128, 256, 512, 1024] # number of nodes\n",
    "        self.k = (3, 3) # kernal size\n",
    "        self.s = (2, 2) # stride (= pooling size)\n",
    "        \n",
    "        \n",
    "        self.conv_block1 = InitConvBlock(self.n[0], self.k)\n",
    "        self.conv_block1_2 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block1_3 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block1_4 = ConvBlock(self.n[0], self.k)\n",
    "        self.pool1 = layers.Conv2D(self.n[1], self.s, self.s)\n",
    "        \n",
    "        self.conv_block2 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block2_2 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block2_3 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block2_4 = ConvBlock(self.n[1], self.k)\n",
    "        self.pool2 = layers.Conv2D(self.n[2], self.s, self.s)\n",
    "        \n",
    "        self.conv_block3 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block3_2 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block3_3 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block3_4 = ConvBlock(self.n[2], self.k)\n",
    "        self.pool3 = layers.Conv2D(self.n[3], self.s, self.s)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_2 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_3 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_4 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block4_5 = ConvBlock(self.n[3], self.k)\n",
    "        self.pool4 = layers.Conv2D(self.n[4], self.s, self.s)\n",
    "        \n",
    "        self.conv_bottom = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_2 = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_3 = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_4 = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_5 = ConvBlock(self.n[4], self.k)\n",
    "        self.conv_bottom_6 = ConvBlock(self.n[4], self.k)\n",
    "        \n",
    "        self.deconv_block1 = layers.UpSampling2D()\n",
    "        self.conv_block_r1 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_2 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_3 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_4 = ConvBlock(self.n[3], self.k)\n",
    "        self.conv_block_r1_5 = ConvBlock(self.n[3], self.k)\n",
    "        \n",
    "        self.deconv_block2 = layers.UpSampling2D()\n",
    "        self.conv_block_r2 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block_r2_2 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block_r2_3 = ConvBlock(self.n[2], self.k)\n",
    "        self.conv_block_r2_4 = ConvBlock(self.n[2], self.k)\n",
    "        \n",
    "        self.deconv_block3 = layers.UpSampling2D()\n",
    "        self.conv_block_r3 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block_r3_2 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block_r3_3 = ConvBlock(self.n[1], self.k)\n",
    "        self.conv_block_r3_4 = ConvBlock(self.n[1], self.k)\n",
    "        \n",
    "        self.deconv_block4 = layers.UpSampling2D()\n",
    "        self.conv_block_r4 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block_r4_2 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block_r4_3 = ConvBlock(self.n[0], self.k)\n",
    "        self.conv_block_r4_4 = ConvBlock(self.n[0], self.k)\n",
    "        \n",
    "        self.padding = layers.ZeroPadding2D(((1, 0), (0, 1)))\n",
    "        self.output_conv = layers.Conv2D(1, (1, 1), activation='sigmoid')\n",
    "        \n",
    "    def call(self, inp, TRAINING=True):\n",
    "        inp = inp/255\n",
    "        \n",
    "        conv1 = self.conv_block1(inp, TRAINING)\n",
    "        conv1 = self.conv_block1_2(conv1, TRAINING)\n",
    "        conv1 = self.conv_block1_3(conv1, TRAINING)\n",
    "        conv1 = self.conv_block1_4(conv1, TRAINING)\n",
    "        pooled1 = self.pool1(conv1)\n",
    "        \n",
    "        conv2 = self.conv_block2(pooled1, TRAINING)\n",
    "        conv2 = self.conv_block2_2(conv2, TRAINING)\n",
    "        conv2 = self.conv_block2_3(conv2, TRAINING)\n",
    "        conv2 = self.conv_block2_4(conv2, TRAINING)\n",
    "        pooled2 = self.pool2(conv2)\n",
    "        \n",
    "        conv3 = self.conv_block3(pooled2, TRAINING)\n",
    "        conv3 = self.conv_block3_2(conv3, TRAINING)\n",
    "        conv3 = self.conv_block3_3(conv3, TRAINING)\n",
    "        conv3 = self.conv_block3_4(conv3, TRAINING)\n",
    "        pooled3 = self.pool3(conv3)\n",
    "        \n",
    "        conv4 = self.conv_block4(pooled3, TRAINING)\n",
    "        conv4 = self.conv_block4_2(conv4, TRAINING)\n",
    "        conv4 = self.conv_block4_3(conv4, TRAINING)\n",
    "        conv4 = self.conv_block4_4(conv4, TRAINING)\n",
    "        conv4 = self.conv_block4_5(conv4, TRAINING)\n",
    "        pooled4 = self.pool4(conv4)\n",
    "        \n",
    "        bottom = self.conv_bottom(pooled4, TRAINING)\n",
    "        bottom = self.conv_bottom_2(bottom, TRAINING)\n",
    "        bottom = self.conv_bottom_3(bottom, TRAINING)\n",
    "        bottom = self.conv_bottom_4(bottom, TRAINING)\n",
    "        bottom = self.conv_bottom_5(bottom, TRAINING)\n",
    "        bottom = self.conv_bottom_6(bottom, TRAINING)\n",
    "        \n",
    "        deconv1 = self.padding(self.deconv_block1(bottom))\n",
    "        deconv1 = layers.concatenate([deconv1, conv4])\n",
    "        deconv1 = self.conv_block_r1(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_2(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_3(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_4(deconv1, TRAINING)\n",
    "        deconv1 = self.conv_block_r1_5(deconv1, TRAINING)\n",
    "        \n",
    "        deconv2 = self.deconv_block2(deconv1)\n",
    "        deconv2 = layers.concatenate([deconv2, conv3])\n",
    "        deconv2 = self.conv_block_r2(deconv2, TRAINING)\n",
    "        deconv2 = self.conv_block_r2_2(deconv2, TRAINING)\n",
    "        deconv2 = self.conv_block_r2_3(deconv2, TRAINING)\n",
    "        deconv2 = self.conv_block_r2_4(deconv2, TRAINING)\n",
    "        \n",
    "        deconv3 = self.deconv_block3(deconv2)\n",
    "        deconv3 = layers.concatenate([deconv3, conv2])\n",
    "        deconv3 = self.conv_block_r3(deconv3, TRAINING)\n",
    "        deconv3 = self.conv_block_r3_2(deconv3, TRAINING)\n",
    "        deconv3 = self.conv_block_r3_3(deconv3, TRAINING)\n",
    "        deconv3 = self.conv_block_r3_4(deconv3, TRAINING)\n",
    "        \n",
    "        deconv4 = self.deconv_block4(deconv3)\n",
    "        deconv4 = layers.concatenate([deconv4, conv1])\n",
    "        deconv4 = self.conv_block_r4(deconv4, TRAINING)\n",
    "        deconv4 = self.conv_block_r4_2(deconv4, TRAINING)\n",
    "        deconv4 = self.conv_block_r4_3(deconv4, TRAINING)\n",
    "        deconv4 = self.conv_block_r4_4(deconv4, TRAINING)\n",
    "        \n",
    "        return self.output_conv(deconv4)*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='mae', optimizer=opt)\n",
    "# model.fit(train_dataset, epochs = 200,\n",
    "#           callbacks = callbacks, validation_data=val_dataset,\n",
    "#           steps_per_epoch = 100, validation_steps=30,\n",
    "#           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_schedule = optimizers.schedules.ExponentialDecay(LEARNING_RATE, DECAY_STEPS, DECAY_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 2674/2674 [00:00<00:00, 3146.46it/s]\n"
     ]
    }
   ],
   "source": [
    "test_path = './data/test'\n",
    "test_files = sorted(glob.glob(test_path + '/*.npy'))\n",
    "\n",
    "X_test = []\n",
    "\n",
    "for file in tqdm(test_files, desc = 'test'):\n",
    "    data = np.load(file)\n",
    "    X_test.append(data)\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-45a9f0f6c68f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "tf.keras.callbacks.Callback(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InverseTimeDecay' object has no attribute 'set_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-af060cb59f96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         model.fit(train_dataset, epochs = 200,\n\u001b[1;32m     26\u001b[0m                   \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                   verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m             steps=data_handler.inferred_steps)\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, callbacks, add_history, add_progbar, model, **params)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'InverseTimeDecay' object has no attribute 'set_model'"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    with strategy.scope():\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        lr_schedule = optimizers.schedules.InverseTimeDecay(LEARNING_RATE, DECAY_STEPS, DECAY_RATE, staircase=True)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath='./models/%s_%d.h5' % (BEST_PATH, i),\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss',\n",
    "                verbose=1),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,  patience=20),\n",
    "            lr_schedule,\n",
    "            tensorboard_callback\n",
    "        ]\n",
    "        model = UNet()\n",
    "        opt = tf.optimizers.Adam(learning_rate=lr_schedule, epsilon=EPSILON)\n",
    "        model.compile(loss='mae', optimizer=opt)\n",
    "#         model.predict(X_test.astype('float32')[:2, ...])\n",
    "#         model.load_weights('./models/DeepResUNet_%d.h5' % i)\n",
    "        model.fit(train_dataset, epochs = 200,\n",
    "                  callbacks = callbacks, validation_data=val_dataset,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: 2.76247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in range(5):\n",
    "    model.load_weights('./models/DeepResUNet_%d.h5' % i)\n",
    "    pred.append(model.predict(X_test.astype('float32')))\n",
    "pred = np.array(pred).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:,1:] = pred.reshape(-1, 14400).astype(int)\n",
    "submission.to_csv('./results/Dacon_DeepResUNet_3.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
