{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.002\n",
    "EPSILON = 1e-06\n",
    "BEST_PATH = './models/ResUNet.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = glob.glob('./data/train/*.npy')\n",
    "train_files = shuffle(train_files, random_state=3101)\n",
    "len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator():\n",
    "    for file in train_files:\n",
    "        dataset = np.load(file)\n",
    "        target= dataset[:,:,-1].reshape(120,120,1)\n",
    "        remove_minus = np.where(target < 0, 0, target)\n",
    "        feature = dataset[:,:,:4]\n",
    "\n",
    "        yield (feature, remove_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = int(len(train_files)*.7)\n",
    "\n",
    "full_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([120,120,4]),tf.TensorShape([120,120,1])))\n",
    "\n",
    "train_dataset = full_dataset.take(NUM_TRAIN)\n",
    "train_dataset = train_dataset.batch(64).prefetch(1)\n",
    "val_dataset = full_dataset.skip(NUM_TRAIN)\n",
    "val_dataset = val_dataset.batch(64).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = plt.cm.get_cmap('RdBu')\n",
    "color_map = color_map.reversed()\n",
    "image_sample = np.load(train_files[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(image_sample[:, :, i], cmap=color_map)\n",
    "\n",
    "plt.subplot(1,5,5)\n",
    "plt.imshow(image_sample[:,:,-1], cmap = color_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitConvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super(InitConvBlock, self).__init__()\n",
    "        self.f = filters\n",
    "        self.k = kernel_size\n",
    "        \n",
    "        self.initconv = layers.Conv2D(self.f, (1, 1), kernel_initializer='he_normal', padding='same')\n",
    "        \n",
    "        self.batch1 = layers.BatchNormalization()\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv2 = layers.Conv2D(self.f, self.k, kernel_initializer='he_normal', padding='same')\n",
    "        \n",
    "    def call(self, inp, TRAINING):\n",
    "        \n",
    "        inp = self.initconv(inp)\n",
    "        shortcut = inp\n",
    "        inp = self.conv2(self.activation1(self.batch1(inp, training=TRAINING)))\n",
    "        inp = layers.add([shortcut, inp])\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.f = filters\n",
    "        self.k = kernel_size\n",
    "        \n",
    "        self.initconv = layers.Conv2D(self.f, (1, 1), kernel_initializer='he_normal', padding='same')\n",
    "        \n",
    "        self.batch1 = layers.BatchNormalization()\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv1 = layers.Conv2D(self.f, self.k, kernel_initializer='he_normal', padding='same')\n",
    "        \n",
    "        self.batch2 = layers.BatchNormalization()\n",
    "        self.activation2 = layers.Activation(tf.nn.leaky_relu)\n",
    "        self.conv2 = layers.Conv2D(self.f, self.k, kernel_initializer='he_normal', padding='same')\n",
    "        \n",
    "    def call(self, inp, TRAINING):\n",
    "        \n",
    "        inp = self.initconv(inp)\n",
    "        shortcut = inp\n",
    "        inp = self.conv1(self.activation1(self.batch1(inp, training=TRAINING)))\n",
    "        inp = self.conv2(self.activation2(self.batch2(inp, training=TRAINING)))\n",
    "        inp = layers.add([shortcut, inp])\n",
    "                \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.f = filters\n",
    "        self.k = kernel_size\n",
    "        self.s = strides\n",
    "        \n",
    "        self.deconv1 = layers.Conv2DTranspose(self.f, self.k, kernel_initializer='he_normal', strides=self.s, padding='same')\n",
    "        self.activation1 = layers.Activation(tf.nn.leaky_relu)\n",
    "        \n",
    "    def call(self, inp):\n",
    "        \n",
    "        inp = self.activation1(self.deconv1(inp))\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(Model):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n = [64, 128, 256, 512, 1024] # number of nodes\n",
    "        self.k = (3, 3) # kernal size\n",
    "        self.s = (2, 2) # stride (= pooling size)\n",
    "        \n",
    "        \n",
    "        self.conv_block1 = InitConvBlock(self.n[0], self.k)\n",
    "        self.pool1 = layers.Conv2D(self.n[1], self.s, self.s)\n",
    "        \n",
    "        self.conv_block2 = ConvBlock(self.n[1], self.k)\n",
    "        self.pool2 = layers.Conv2D(self.n[2], self.s, self.s)\n",
    "        \n",
    "        self.conv_block3 = ConvBlock(self.n[2], self.k)\n",
    "        self.pool3 = layers.Conv2D(self.n[3], self.s, self.s)\n",
    "        \n",
    "        self.conv_block4 = ConvBlock(self.n[3], self.k)\n",
    "        self.pool4 = layers.Conv2D(self.n[4], self.s, self.s)\n",
    "        \n",
    "        self.conv_bottom = ConvBlock(self.n[4], self.k)\n",
    "        \n",
    "        self.deconv_block1 = DeconvBlock(self.n[3], self.k, self.s)\n",
    "        self.conv_block_r1 = ConvBlock(self.n[3], self.k)\n",
    "        \n",
    "        self.deconv_block2 = DeconvBlock(self.n[2], self.k, self.s)\n",
    "        self.conv_block_r2 = ConvBlock(self.n[2], self.k)\n",
    "        \n",
    "        self.deconv_block3 = DeconvBlock(self.n[1], self.k, self.s)\n",
    "        self.conv_block_r3 = ConvBlock(self.n[1], self.k)\n",
    "        \n",
    "        self.deconv_block4 = DeconvBlock(self.n[0], self.k, self.s)\n",
    "        self.conv_block_r4 = ConvBlock(self.n[0], self.k)\n",
    "        \n",
    "        self.padding = layers.ZeroPadding2D(((1, 0), (0, 1)))\n",
    "        self.output_conv = layers.Conv2D(1, (1, 1), activation='sigmoid')\n",
    "        \n",
    "    def call(self, inp, TRAINING=True):\n",
    "        inp = inp/255\n",
    "        \n",
    "        conv1 = self.conv_block1(inp, TRAINING)\n",
    "        pooled1 = self.pool1(conv1)\n",
    "        \n",
    "        conv2 = self.conv_block2(pooled1, TRAINING)\n",
    "        pooled2 = self.pool2(conv2)\n",
    "        \n",
    "        conv3 = self.conv_block3(pooled2, TRAINING)\n",
    "        pooled3 = self.pool3(conv3)\n",
    "        \n",
    "        conv4 = self.conv_block4(pooled3, TRAINING)\n",
    "        pooled4 = self.pool4(conv4)\n",
    "        \n",
    "        bottom = self.conv_bottom(pooled4, TRAINING)\n",
    "        \n",
    "        deconv1 = self.padding(self.deconv_block1(bottom))\n",
    "        deconv1 = layers.concatenate([deconv1, conv4])\n",
    "        deconv1 = self.conv_block_r1(deconv1, TRAINING)\n",
    "        \n",
    "        deconv2 = self.deconv_block2(deconv1)\n",
    "        deconv2 = layers.concatenate([deconv2, conv3])\n",
    "        deconv2 = self.conv_block_r2(deconv2, TRAINING)\n",
    "        \n",
    "        deconv3 = self.deconv_block3(deconv2)\n",
    "        deconv3 = layers.concatenate([deconv3, conv2])\n",
    "        deconv3 = self.conv_block_r3(deconv3, TRAINING)\n",
    "        \n",
    "        deconv4 = self.deconv_block4(deconv3)\n",
    "        deconv4 = layers.concatenate([deconv4, conv1])\n",
    "        deconv4 = self.conv_block_r4(deconv4, TRAINING)\n",
    "        \n",
    "        return self.output_conv(deconv4)*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=BEST_PATH,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,  patience=20)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(learning_rate=LEARNING_RATE, epsilon=EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mae', optimizer=opt)\n",
    "model.fit(train_dataset, epochs = 200,\n",
    "          callbacks = callbacks, validation_data=val_dataset,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(BEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './data/test'\n",
    "test_files = sorted(glob.glob(test_path + '/*.npy'))\n",
    "\n",
    "X_test = []\n",
    "\n",
    "for file in tqdm(test_files, desc = 'test'):\n",
    "    data = np.load(file)\n",
    "    X_test.append(data)\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:,1:] = pred.reshape(-1, 14400).astype(int)\n",
    "submission.to_csv('./results/Dacon_UNet_1.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
